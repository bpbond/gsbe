---
title: "2-gsbe"
author: "bbl"
format:
  html:
    embed-resources: true
    code-fold: true
toc: true
editor: visual
---

```{r setup}
#| include: false

K_FOLD <- 5

# Data processing and visualization packages
library(dplyr)
library(tidyr)
library(ggplot2)
theme_set(theme_bw())
library(readr)
library(lubridate)
library(arrow)

# Main analysis packages
library(ranger)
library(vivid)
library(broom)
library(car)

# Specialized packages used only for testing alternatives
library(party)
library(xgboost)
library(geodist)
library(spatialRF)

source("utilities.R")
```

# Background and science questions

(short background)

We ask four questions of increasing specificity:

1.  Does SPEI -- incorporating the statistical deviations of precipitation and evaporative demand in a single index -- add *Rs* predictive information beyond that of climate, LAI, vegetation, and remotely-derived productivity?

2.  Does the previous year's SPEI value have *Rs* predictive value? I.e., is there a memory effect?

3.  Specifically for *Rs* observations in a non-drought year *following one or more drought years*, does the previous year's SPEI have predictive value?

4.  Is there a 'Birch effect' in the observational record, in which non-drought *Rs* is higher than it otherwise would be when preceded by a drought?

# 0. Prep work and base models

```{r read-data}

dat <- read_csv("srdb_joined_data.csv", show_col_types = FALSE)

message(nrow(dat), " rows of data")

dat %>% 
    filter(is.na(Rs_growingseason) | (Rs_growingseason > 0 & Rs_growingseason < 20)) %>% 
    filter(is.na(Rs_annual) | (Rs_annual > 0 & Rs_annual < 4000)) %>% 
    filter(is.na(Rh_annual) | (Rh_annual > 0 & Rh_annual < 2000)) %>% 
    # Most SRDB records are from forests (~3000), then grasslands (~760), 
    # shrubland (~230), wetland (~210), and desert (~80). We add the next
    # one, savanna (~30), because it might be particularly vulnerable to 
    # a Birch effect, but group everything else into "Other"
    mutate(Ecosystem_type = if_else(Ecosystem_type %in% c("Desert", "Forest", "Grassland",
                                                          "Savanna", "Shrubland", "Wetland"),
                                    true = Ecosystem_type, 
                                    false = "Other",
                                    missing = "Other"),
           # vivid::pdpVars() below needs things as factors not char
           Ecosystem_type = as.factor(Ecosystem_type),
           Soil_type_name = as.factor(Soil_type_name)) ->
    dat_filtered

message("After filtering, ", nrow(dat_filtered), " rows of data")
```
## Collinearity

 `Tair`, `Tsoil_lev1`, and `Tsoil_lev2` are *highly* correlated
with each other (`see 1-gsbe-data-prep`), so drop level 2.

`VWC_lev1` and `VWC_lev2` are *highly* correlated with each
other (see `1-gsbe-data-prep`), so drop level 2.

## Dependent variable distribution

We have three potential dependent variables:

-   `Rs_annual` (annual soil respiration)

-   `Rh_annual` (annual heterotrophic respiration)

-   `Rs_growingseason` (mean growing season soil respiration)

Their distributions are expected to be non-normal, which is a problem for linear models and not ideal for anything. Evaluate possible transformations.

```{r dv-dist}
#| fig-width: 10
#| fig-height: 8

dat_filtered %>% 
    select(Rs_annual, Rh_annual, Rs_growingseason) %>% 
    pivot_longer(everything()) %>% 
    filter(!is.na(value), value > 0) %>% 
    rename(none = value) %>% 
    mutate(log = log(none), sqrt = sqrt(none)) %>% 
    
    pivot_longer(-name, names_to = "Transformation") %>% 
    ggplot(aes(value, color = Transformation)) + 
    geom_density() +
    facet_wrap(~ name + Transformation, scales = "free")

dat_filtered %>% 
    mutate(sqrt_Rs_annual = sqrt(Rs_annual),
           sqrt_Rh_annual = sqrt(Rh_annual),
           sqrt_Rs_growingseason = sqrt(Rs_growingseason)) ->
    dat_filtered
```

**Conclusion: we have a transformation winner: `sqrt()`!**

## Random Forest model: performance

We are using the `ranger` package.

Wright, M. N. and Ziegler, A.: Ranger: A fast implementation of random forests for high dimensional data in C++ and R, J. Stat. Softw., 77, https://doi.org/10.18637/jss.v077.i01, 2017.

```{r base-rf}
#| fig-width: 10
#| fig-height: 6
#| cache: true

# Fit random forest model and test against validation data
# The independent variable is in the first column
fit_and_test_rf <- function(x_train, x_val) {
    f <- as.formula(paste(colnames(x_train)[1], "~ ."))
    rf <- ranger(formula = f, data = x_train, importance = "impurity")
    # use importance = "permutation" for importance_pvalues()
    training_rmse <- rmse(predict(rf, data = x_train)$predictions, pull(x_train[1]))
        
    if(!is.null(x_val)) {
        val_preds <- predict(rf, data = x_val)$predictions
        val_obs <- pull(x_val[1])
        validation_r2 <- r2(preds = val_preds, obs = val_obs)
        validation_rmse <- rmse(preds = val_preds, obs = val_obs)
    } else {
        validation_r2 <- NA_real_
        validation_rmse <- NA_real_
    }
    
    tibble(training_r2 = rf$r.squared, 
           training_rmse = training_rmse,
           validation_r2 = validation_r2,
           validation_rmse = validation_rmse,
           # PBE_importance isn't used until section 4's sensitivity test
           pbe_importance = rank(importance(rf))["PBE"])
}

# Base test dataset (all three d.v.'s)
dat_filtered %>% 
    select(sqrt_Rs_annual, sqrt_Rh_annual, sqrt_Rs_growingseason,
           # Tair, Tsoil_lev1, and Tsoil_lev2 are *highly* correlated
           # with each other (see 1-gsbe-data-prep), so drop level 2
           Tair, Tsoil_lev1, #Tsoil_lev2,
           # VWC_lev1 and VWC_lev2 are *highly* correlated with each
           # other (see 1-gsbe-data-prep), so drop level 2
           Precip, VWC_lev1, #VWC_lev2, 
           LAI_high, LAI_low, Ecosystem_type,
           starts_with("SPEI"),
           MODIS_NPP, MODIS_GPP,
           Veg_type_hi, Veg_type_low, Soil_type_name) ->
    dat_base_all

# Base test dataset without MODIS or SPEI data
dat_base_all %>% 
    select(-MODIS_NPP, -MODIS_GPP, -starts_with("SPEI")) ->
    dat_base_no_modispei

# Base test dataset (Rs_annual only)
dat_base_no_modispei %>% 
    select(-sqrt_Rh_annual, -sqrt_Rs_growingseason) %>% 
    filter(complete.cases(.)) -> 
    dat_base_Rs_annual

message("The test complete-cases dataset has ", dim(dat_base_Rs_annual)[1],
        " rows and ", dim(dat_base_Rs_annual)[2], " columns")
message("Full-data model performance:")
rf_all_data <- ranger(sqrt_Rs_annual ~ ., data = dat_base_Rs_annual, importance = "impurity")
print(rf_all_data)

message("Random forest predictor importance and Altmann p-values:")
imps <- sort(rf_all_data$variable.importance, decreasing = TRUE)
importance_pvalues(rf_all_data, "altmann", 
                   num.permutations = 100,
                   formula = sqrt_Rs_annual ~ ., 
                   data = dat_base_Rs_annual)

# Top three variables...
pdpVars(data = dat_base_Rs_annual, 
        fit = rf_all_data, 
        response = "sqrt_Rs_annual",
        vars = names(imps)[1:3])
# ...and the next three
pdpVars(data = dat_base_Rs_annual, 
        fit = rf_all_data, 
        response = "sqrt_Rs_annual",
        vars = names(imps)[4:6])

# k-fold cross-validation for all dependent variables
message("k-fold cross-validation:")

dvs <- c("sqrt_Rs_annual", "sqrt_Rh_annual", "sqrt_Rs_growingseason")
preds <- setdiff(colnames(dat_base_no_modispei), dvs)
kf_out <- list()
for(dv in dvs) {
    # Construct dataset and do the k-fold
    x <- dat_base_no_modispei[c(dv, preds)]
    x <- x[complete.cases(x),]
    do_k_fold(x, fit_and_test_rf) %>% 
        select(-pbe_importance) %>%  # not used here
        mutate(dv = dv, n = nrow(x)) ->
        kf_out[[dv]]
}
bind_rows(kf_out) %>% 
    group_by(dv) %>% 
    summarise(across(everything(), \(x) mean(x, digits = 3)), .groups = "drop") %>% 
    mutate(k = K_FOLD) -> # just so it's "10" or whatever as expected
    kf_out 
print(kf_out)

```

**Side note: model performance is linearly related to dataset size! Huh.**

```{r side-note}
ggplot(kf_out, aes(n, validation_r2)) + 
    geom_point() + 
    geom_smooth(method = lm, formula = y~x) +
    ggtitle("Validation R2 for Rs_annual, Rh_annual, and Rs_growingseason")
```

## How important are the MODIS data?

The MODIS data only start in 2001 (see `gsbe-data-prep` output), so requiring them in a model cuts off a lot of early data – on the order of 20% of the dataset. Are they needed?

```{r test-modis}
#| cache: true

dat_base_all %>% 
    select(-sqrt_Rh_annual, -sqrt_Rs_growingseason, -starts_with("SPEI")) %>% 
    filter(complete.cases(.)) -> 
    dat_base_test_modis

rf_modis <- ranger(sqrt_Rs_annual ~ ., data = dat_base_test_modis, importance = "impurity")
print(rf_modis)

message("Model *with* MODIS data: OOB R2 = ", round(rf_modis$r.squared, 4))
message("Model *without* MODIS data: OOB R2 = ", round(rf_all_data$r.squared, 4))

importance_pvalues(rf_modis, "altmann", 
                   num.permutations = 100,
                   formula = sqrt_Rs_annual ~ ., 
                   data = dat_base_test_modis)
```

**Conclusion: it doesn't seem like they're needed.**

## Roads not taken

### Test spatial Random Forest

Spatial autocorrelation wouldn't change the fit of the model, but it could 
bias the variable importance metrics. Evaluate.

```{r spatial-rf}
# This needs the newest version 1.1.5, not yet on CRAN
# remotes::install_github(repo = "blasbenito/spatialRF",
#   ref = "main", force = TRUE, quiet = TRUE)
stopifnot(packageVersion("spatialRF") >= "1.1.5")

dat_base_no_modispei %>% 
    bind_cols(select(dat_filtered, Longitude, Latitude)) %>% 
    select(-sqrt_Rh_annual, -sqrt_Rs_growingseason) %>% 
    filter(complete.cases(.)) -> 
    dat_spatial_test

# Compute distance between all pairs of points
dm <- geodist(select(dat_spatial_test, Longitude, Latitude), measure = "geodesic")
dat_spatial_test$Longitude <- dat_spatial_test$Latitude <- NULL
predictors <- setdiff(colnames(dat_spatial_test), colnames(dat_spatial_test)[1])
spatialRF::rf(data = dat_spatial_test, 
              dependent.variable.name = colnames(dat_spatial_test)[1],
              predictor.variable.names = predictors,
              distance.matrix = dm)
```

**Conclusion: no strong evidence of spatial autocorrelation.**

### Test XGBoost

Is there any reason to think that this popular algorithm would out-perform
our Random Forest approach, and/or give different answers?

```{r xgboost}
fit_and_test_xgboost <- function(x_train, x_val) {
    m_xgb <- xgboost(x_train[-1], x_train[1])
    pred_train <- predict(m_xgb, newdata = x_train)
    pred_val <- predict(m_xgb, newdata = x_val)
    tibble(training_r2 = r2(pred_train, pull(x_train[1])),
           validation_r2 = r2(pred_val, pull(x_val[1])))
}

m_xgb <- xgboost(dat_base_Rs_annual[-1], dat_base_Rs_annual[1])
message("Variable importance of XGBoost model:")
as_tibble(xgb.importance(m_xgb))

message("k-fold cross-validation results:")
do_k_fold(dat_base_Rs_annual, fit_and_test_xgboost) %>% 
    summary()
```

Seems like xgboost is over-fitting to the training data. Note however that

> Unlike random forests, GBMs can have high variability in accuracy dependent 
on their hyperparameter settings [(Probst, Bischl, and Boulesteix 2018)](https://arxiv.org/abs/1802.09596) So tuning can require much more 
strategy than a random forest model.

**Conclusion: performance and variable importance seem similar to `ranger`.**

### Test neural network

We use the `neuralnet` package for this. I experimented with the hidden layer
topology and threshold settings to find a configuration that reliably converges
with these data. Note: fitting this is *slow*.

```{r neuralnet}
#| cache: true
#| eval: false

# eval:false above b/c this is SO SLOW

library(neuralnet)

fit_and_test_nn <- function(x_train, x_val) {
    # Algorithm is not guaranteed to converge; if this happens, returns NAs
    out <- tibble(training_r2 = NA_real_, validation_r2 = NA_real_)
    try({
        m_nn <- neuralnet(sqrt_Rs_annual ~ ., data = x_train,
                          hidden = c(12, 6), rep = 2, threshold = 0.4)
        pred_train <- predict(m_nn, newdata = x_train)
        pred_val <- predict(m_nn, newdata = x_val)
        out <- tibble(training_r2 = r2(pred_train, pull(x_train[1])),
                      validation_r2 = r2(pred_val, pull(x_val[1])))
    })
    return(out)
}

# Rescale the variables and change factors to numeric
x <- dat_base_Rs_annual
nums <- sapply(x, is.numeric)
for(col in names(nums)[nums]) x[col] <- scale(x[col])
fcts <- sapply(x[-1], is.factor)
for(col in names(fcts)[fcts]) x[col] <- as.numeric(pull(x[col]))
#saveRDS(x, "nn_topology/x.RDS")

# Overall model
m_nn <- neuralnet(sqrt_Rs_annual ~ ., data = x, hidden = c(12, 6), rep = 2, threshold = 0.4)
message("R2 of two-layer model: ", round(r2(predict(m_nn, newdata = x), x$sqrt_Rs_annual), 3))
m_nn <- neuralnet(sqrt_Rs_annual ~ ., data = x, hidden = c(24, 12, 6), rep = 2, threshold = 0.4)
message("R2 of three-layer model: ", round(r2(predict(m_nn, newdata = x), x$sqrt_Rs_annual), 3))

message("k-fold cross-validation results:")
do_k_fold(x, fit_and_test_nn) %>% 
    summary()
```

With a complex topology (e.g., three hidden layers) the training R2
can be cranked up to 0.9+, but it exhibits severe overfitting.

**Conclusion: I'm not an expert here! But, no evidence that this
will be a magic bullet.**

## Linear regression model: performance

```{r base-lm}
#| fig-width: 10
#| fig-height: 6

# Fit linear model and test against validation data
# The dependent variable is in the first column
fit_and_test_lm <- function(x_train, x_val, m) {
    pred_train <- predict(m, newdata = x_train)
    pred_val <- predict(m, newdata = x_val)
    tibble(training_r2 = r2(pred_train, pull(x_train[1])),
           training_rmse = rmse(pred_train, pull(x_train[1])),
           validation_r2 = r2(pred_val, pull(x_val[1])),
           validation_rmse = rmse(pred_val, pull(x_val[1])))
}

message("Fitting Rs_annual linear model...")
m_Rs <- lm(sqrt_Rs_annual ~ Tair + Tsoil_lev1 + 
            VWC_lev1 + I(VWC_lev1 ^ 2) + 
            Ecosystem_type + LAI_high * LAI_low + 
            Veg_type_hi + Veg_type_low + Soil_type_name,
        data = dat_base_Rs_annual)
m_Rs_reduced <- MASS::stepAIC(m_Rs, trace = FALSE)
summary(m_Rs_reduced)
par(mfrow = c(2, 2))
plot(m_Rs_reduced)

message("k-fold cross-validation results:")
lm_rhs_formula <- as.character(formula(m_Rs))[3]
kf_out <- list()
for(dv in dvs) {
    # Pull out the rhs of the formula and paste iv (independent variable) in front
    f <- as.formula(paste(dv, "~", lm_rhs_formula))
    # Construct dataset and call lm() once to give us a base model
    x <- dat_base_no_modispei[c(dv, preds)]
    x <- x[complete.cases(x),]
    m <- lm(f, data = x)
    kf_out[[dv]] <- do_k_fold(x, f = fit_and_test_lm, m = m) %>% 
        mutate(dv = dv, n = nrow(x))
}
bind_rows(kf_out) %>% 
    group_by(dv) %>% 
    summarise(across(everything(), mean), .groups = "keep") %>% 
    mutate(across(everything(), \(x) round(x, digits = 3))) %>% 
    ungroup() %>% 
    mutate(k = K_FOLD) # just so it's "10" or whatever as expected

# Directly compare lm and rf predictions
tibble(Ecosystem_type = dat_base_Rs_annual$Ecosystem_type,
       lm_prediction = predict(m_Rs_reduced),
       rf_prediction = predict(rf_all_data, data = dat_base_Rs_annual)$predictions) %>% 
    ggplot(aes(lm_prediction, rf_prediction)) + 
    geom_point(alpha = 0.25) + 
    geom_abline() + 
    geom_smooth(method = "lm", formula = y ~ x) +
    ggtitle("Linear model versus Random Forest predictions") + 
    facet_wrap(~Ecosystem_type, scales = "free")
```

**Conclusions:**

-   **Linear model performance is less than half of Random Forest**

-   **Variable importance is similar**

# 1. Predictive value of SPEI

```{r spei-value}
#| cache: true

dat_base_all %>% 
    select(-sqrt_Rh_annual, -sqrt_Rs_growingseason,
           -MODIS_NPP, -MODIS_GPP,
           -starts_with("SPEI24")) %>% 
    filter(complete.cases(.)) -> 
    dat_test_spei

# remove the "_y1" columns
yr1cols <- grep("_y1$", colnames(dat_test_spei))
dat_test_spei_current <- dat_test_spei[-yr1cols]

message("Testing Random Forest...")
rf_spei <- ranger(sqrt_Rs_annual ~ ., data = dat_test_spei_current, importance = "impurity")
print(rf_spei)
importance_pvalues(rf_spei, "altmann", 
                   num.permutations = 100,
                   formula = sqrt_Rs_annual ~ ., 
                   data = dat_test_spei_current)

message("Model *with* SPEI data: OOB R2 = ", round(rf_spei$r.squared, 4))
message("Model *without* SPEI data: OOB R2 = ", round(rf_all_data$r.squared, 4))

message("Testing linear model...")
m_Rs_spei <- lm(sqrt_Rs_annual ~ (Tair + Tsoil_lev1 + 
            VWC_lev1 + I(VWC_lev1 ^ 2)) * 
               SPEI12 +
            Ecosystem_type + LAI_high * LAI_low + 
            Veg_type_hi + Veg_type_low + Soil_type_name,
        data = dat_test_spei_current)
m_Rs_spei_reduced <- MASS::stepAIC(m_Rs_spei, trace = FALSE)
car::Anova(m_Rs_spei_reduced, type = "III")

message("Model *with* SPEI data: training R2 = ", lm_r2(m_Rs_spei_reduced))
message("Model *without* SPEI data: training R2 = ", lm_r2(m_Rs_reduced))
message("ANOVA model comparison p-value: ", 
        anova_p_twomodels(m_Rs_spei_reduced, m_Rs_reduced))
```

**Conclusions:**

-   **SPEI impact on Random Forest model is small, but there is a performance improvement.**

-   **SPEI *does* have effects in the linear model: significant interactions with many variables, significant difference between models with and without SPEI, and model R2 jumps modestly.**

# 2. Predictive value of previous-year SPEI

```{r prev-spei-value}

# Use the dat_test_spei dataset from above

message("Testing Random Forest...")
rf_spei_y1 <- ranger(sqrt_Rs_annual ~ ., data = dat_test_spei, importance = "impurity")
print(rf_spei_y1)
message("Model *with* previous-year SPEI data: OOB R2 = ", round(rf_spei_y1$r.squared, 4))
message("Model *without* previous-year SPEI data: OOB R2 = ", round(rf_all_data$r.squared, 4))

importance_pvalues(rf_spei_y1, "altmann", 
                   num.permutations = 100,
                   formula = sqrt_Rs_annual ~ ., 
                   data = dat_test_spei)

message("Testing linear model...")
m_Rs_spei_y1 <- lm(sqrt_Rs_annual ~ (Tair + Tsoil_lev1 + 
            VWC_lev1 + I(VWC_lev1 ^ 2)) * 
               (SPEI12 + SPEI12_y1) +
            Ecosystem_type + LAI_high * LAI_low + 
            Veg_type_hi + Veg_type_low + Soil_type_name,
        data = dat_test_spei)
m_Rs_spei_y1_reduced <- MASS::stepAIC(m_Rs_spei_y1, trace = FALSE)
car::Anova(m_Rs_spei_y1_reduced, type = "III")

message("Model *with* previous-year SPEI data: training R2 = ", lm_r2(m_Rs_spei_y1_reduced))
message("Model *without* previous-year SPEI data: training R2 = ", lm_r2(m_Rs_spei_reduced))
message("ANOVA model comparison p-value: ", 
        anova_p_twomodels(m_Rs_spei_y1_reduced, m_Rs_spei_reduced))
```

**BUT there's a problem with the above.** 
Fit a model with *random* previous-year SPEI data:

```{r random-speiyr1}
dat_test_spei %>% 
    mutate(SPEI12_y1 = runif(n())) ->
    dat_test_spei_random

m_spei_random1 <- lm(formula(m_Rs_spei_y1), data = dat_test_spei_random)
m_spei_random1_reduced <- MASS::stepAIC(m_spei_random1, trace = FALSE)
car::Anova(m_spei_random1_reduced, type = "III")
```

...and some of the `SPEI_yr1` terms might still be significant, just by chance!

Use an ANOVA to compare the (reduced) models with and without previous-year SPEI:

```{r random-speiyr1-anova}
m_spei_random2 <- lm(sqrt_Rs_annual ~ (Tair + Tsoil_lev1 + 
            VWC_lev1 + I(VWC_lev1 ^ 2)) * 
               (SPEI12) +
            Ecosystem_type + LAI_high * LAI_low + 
            Veg_type_hi + Veg_type_low + Soil_type_name,
        data = dat_test_spei_random)
m_spei_random2_reduced <- MASS::stepAIC(m_spei_random2, trace = FALSE)

anova(m_spei_random1_reduced, m_spei_random2_reduced)
```

**Conclusions:**

-   **SPEI_y1 impact on Random Forest model is...unclear. No performance improvement.**

-   **SPEI_y1 has significant interactions with many variables in the linear model, model comparison ANOVA is significant, and model R2 increases slightly.**

-   **Need to be careful to include null model testing.**

# 3. Value of previous-year SPEI *after drought*

Similar to the previous section but focused on *post-drought observations* 
(was SPEI <= -1 the previous year but now, in year of Rs measurement, 
SPEI is >= 0).

```{r post-drought}
dat_test_spei %>% 
    filter(SPEI12 >= 0 & SPEI12_y1 <= -1) ->
    dat_test_post_drought
```

There are `r nrow(dat_test_post_drought)` Rs data points that meet these post-drought criteria.

```{r post-drought-spei}
message("Linear model:")
m_pd <- lm(formula = formula(m_Rs_spei), data = dat_test_post_drought)
m_pd_reduced <- MASS::stepAIC(m_pd, trace = FALSE)
m_pd_y1 <- lm(formula = formula(m_Rs_spei_y1), data = dat_test_post_drought)
m_pd_y1_reduced <- MASS::stepAIC(m_pd_y1, trace = FALSE)

message("Post-drought model *with* previous-year SPEI: training R2 = ", lm_r2(m_pd_y1_reduced))
message("Post-drought model *without* previous-year SPEI: training R2 = ", lm_r2(m_pd_reduced))
message("ANOVA model comparison p-value: ", 
        anova_p_twomodels(m_pd_y1_reduced, m_pd_reduced))

tibble(sqrt_Rs_annual = dat_test_post_drought$sqrt_Rs_annual, 
       WITHOUT_y1_SPEI = predict(m_pd),
       WITH_y1_SPEI = predict(m_pd_y1_reduced),
       Ecosystem_type = dat_test_post_drought$Ecosystem_type) %>% 
    pivot_longer(starts_with("WITH")) %>% 
    ggplot(aes(sqrt_Rs_annual, value, color = name)) +
    geom_point() + geom_abline() + 
    geom_smooth(method = lm, formula = y~x, se = FALSE, linetype = 2) +
    xlab("Observed Rs_annual") + ylab("Predicted Rs_annual")
```

Null test: random SPEI_y1.

```{r null-no-change}
dat_test_post_drought %>% 
    mutate(SPEI12_y1 = runif(n())) ->
    dat_test_post_drought_random

m_pd_random <- lm(formula = formula(m_pd_y1), data = dat_test_post_drought_random)
m_pd_random_reduced <- MASS::stepAIC(m_pd_random, trace = FALSE)

message("Post-drought model without previous-year SPEI: training R2 = ", lm_r2(m_pd_reduced))
message("Post-drought model with *random* previous-year SPEI: training R2 = ", lm_r2(m_pd_random_reduced))
message("ANOVA model comparison p-value: ", 
        anova_p_twomodels(m_pd_reduced, m_pd_random_reduced))
```

**Conclusions: for post-drought observations, previous-year SPEI1 has a big effect on prediction.**

# 4. Testing for a Birch effect

Are post-drought observations 'different'?

Define a PBE ("potential Birch effect") flag using the post-drought
conditions above: PBE <= -1 the previous year and PBE >0 = the year of
Rs measurement.

## PBE importance and interactions

```{r test-pbe}
#| fig-width: 10
#| fig-height: 6

dat_base_all %>% 
    select(-MODIS_NPP, -MODIS_GPP) -> 
    dat_pbe_base

# First test is using SPEI12, with a <=-1 to >0 as post-drought
dat_pbe_base %>% 
    mutate(PBE = SPEI12_y1 <= -1 & SPEI12 > 0) %>% 
    dplyr::select(-sqrt_Rh_annual, -sqrt_Rs_growingseason, -starts_with("SPEI")) %>% 
    filter(complete.cases(.)) ->
    dat_pbe
dat_pbe %>% 
    select(-PBE) -> 
    dat_no_pbe

message("Fitting RF model with PBE (SPEI12, -1.0) effect...")
rf_pbe <- ranger(sqrt_Rs_annual ~ ., data = dat_pbe, importance = "impurity")
print(rf_pbe)
importance_pvalues(rf_pbe, "altmann", 
                   num.permutations = 100,
                   formula = sqrt_Rs_annual ~ ., 
                   data = dat_pbe)
rf_no_pbe <- ranger(sqrt_Rs_annual ~ ., data = dat_no_pbe, importance = "impurity")

message("Model *with* PBE flag: OOB R2 = ", round(rf_pbe$r.squared, 4))
message("Model *without* PBE flag: OOB R2 = ", round(rf_no_pbe$r.squared, 4))

message("Fitting linear model with PBE (SPEI12, -1.0) effect...")
m_Rs_PBE <- lm(sqrt_Rs_annual ~ (Tair + Tsoil_lev1 +
            VWC_lev1 + I(VWC_lev1 ^ 2)) * PBE +
            Ecosystem_type + LAI_high * LAI_low + 
            Veg_type_hi + Veg_type_low + Soil_type_name,
        data = dat_pbe)
m_Rs_PBE_reduced <- MASS::stepAIC(m_Rs_PBE, trace = FALSE)
car::Anova(m_Rs_PBE_reduced, type = "III")

message("Fitting Rs_annual linear model without PBE effect...")
m_Rs_no_PBE <- lm(sqrt_Rs_annual ~ (Tair + Tsoil_lev1 +
            VWC_lev1 + I(VWC_lev1 ^ 2)) +
            Ecosystem_type + LAI_high * LAI_low + 
            Veg_type_hi + Veg_type_low + Soil_type_name,
        data = dat_no_pbe)
m_Rs_no_PBE_reduced <- MASS::stepAIC(m_Rs_no_PBE, trace = FALSE)

message("Linear model without PBE flag: training R2 = ", lm_r2(m_Rs_no_PBE_reduced))
message("Linear model with PBE flag: training R2 = ", lm_r2(m_Rs_PBE_reduced))
message("ANOVA model comparison p-value: ", 
        anova_p_twomodels(m_Rs_PBE_reduced, m_Rs_no_PBE_reduced))

tibble(model_with_PBE = predict(m_Rs_PBE),
       model_without_PBE = predict(m_Rs_no_PBE),
       PBE = dat_pbe$PBE, 
       Ecosystem_type = dat_pbe$Ecosystem_type) %>% 
    ggplot(aes(model_without_PBE, model_with_PBE)) + 
    geom_point(aes(color = PBE)) + 
    geom_abline() + 
    facet_wrap(~Ecosystem_type) +
    ggtitle("Linear model: effect of including PBE on sqrt_Rs_annual")

# Extract the significant interactions between PBE and other variables 
message("Interactive PBE effects:")
tidy(m_Rs_PBE_reduced) %>% 
    mutate(model = "Rs_annual") %>% 
    filter(grepl("PBE", term)) %>% 
    separate(term, into = c("interaction", "PBE"), fill = "left", sep = ":") %>% 
    replace_na(list(interaction = "(none)")) %>% 
    select(model, interaction, estimate, p.value) %>% 
    mutate(estimate = round(estimate, 4), p.value = round(p.value, 4)) ->
    PBE_interactions

DT::datatable(PBE_interactions)

# Summary table of significant effect directions
PBE_interactions %>% 
    mutate(signif = p.value < 0.05, 
           effect = case_when(signif & estimate < 0 ~ "Negative",
                              signif & estimate >= 0 ~ "Positive",
                              .default = "")) %>% 
    select(model, interaction, effect) %>%
    pivot_wider(values_from = effect, names_from = model, values_fill = "") %>% 
    knitr::kable(caption = "PBE: significant effect directions")
```

**Conclusions:**

-   **PBE (potential Birch effect: a drought the year before a well-watered year in which Rs was measured) has low importance in a RF model, although its Altmann p-value is marginally significant.**

-   **In a linear model, PBE has a strong effect on the temperature and moisture drivers of soil respiration.**

-   **But, it's inconsistent--there's no clean, unidirectional 'Birch effect.'**

## PBE sensitivity test

Are the above results due to particular choices about what constitutes
a drought (the SPEI cutoff value) and/or the SPEI window (12 or 24 months)?

Here we test the cutoff SPEI values from Table 1 of 
[Keune et al. (2025)](https://www.nature.com/articles/s41597-025-04896-y)
corresponding to "severely dry," "moderately dry," and "mildly dry" conditions.

```{r sensitivity-test}

# These are the three most common fluxes recorded in SRDB
dv_set <- c("sqrt_Rs_annual", "sqrt_Rh_annual", "sqrt_Rs_growingseason")

# The following SPEI values are from Table 1 of Keune et al. (2025)
# https://www.nature.com/articles/s41597-025-04896-y
# and correspond to "severely dry", "moderately dry", and "mildly dry"
# (there are no "extremely dry"=-2.33 values in the dataset)
spei_cutoffs <- c(-1.65, -1.28, -0.84)

# What SPEI window should be used for the cutoffs above?
spei_windows <- c(12, 24) # months

# Model independent variables
predictors <- c("Ecosystem_type",
                "SPEI12",
                "Tair", "Precip",
                "Tsoil_lev1", "VWC_lev1", 
                "Veg_type_hi", "Veg_type_low", 
                "LAI_high", "LAI_low", "Soil_type_name")

# Linear model right hand side
lm_rhs_formula <- as.character(formula(m_Rs_PBE))[3]

test_lm_pbe <- function(x_train, x_val, formula) {
    m <- lm(formula = formula, data = x_train)
    pred_train <- predict(m, newdata = x_train)
    # If data sizes are small (as happens with Rh_annual and spei_cutoff -1.5)
    # there might be a mismatch in available data, causing prediction of
    # validation data to fail
    try(pred_val <- predict(m, newdata = x_val))
    if(exists("pred_val")) {
        validation_r2 <- r2(pred_val, pull(x_val[1]))
        validation_rmse <- rmse(pred_val, pull(x_val[1]))
    } else {
        validation_r2 <- validation_rmse <- NA_real_
    }
    
    # Reduce the model and identify and significant PBE terms remaining
    m_reduced <- MASS::stepAIC(m, trace = FALSE)
    pbe_terms <- tidy(m_reduced) %>% filter(grepl("PBE", term))
    n_pbe_terms <- nrow(pbe_terms)
    n_signif_pbe_terms <- sum(pbe_terms$p.value < 0.05)
    
    # Re-fit model with no PBE term
    x_no_pbe <- x_train
    x_no_pbe$PBE <- 1
    m_no_pbe <- lm(formula = formula, data = x_no_pbe)

    tibble(n_pbe_terms = n_pbe_terms,
           n_signif_pbe_terms = n_signif_pbe_terms,
           pbe_anova = anova_p_twomodels(m, m_no_pbe),
           training_r2 = r2(pred_train, pull(x_train[1])),
           training_rmse = rmse(pred_train, pull(x_train[1])),
           validation_r2 = validation_r2,
           validation_rmse = validation_rmse)
}

do_sensitivity_analysis <- function(x, dv_set, predictors, spei_windows, spei_cutoffs) {
    rf_results <- list()
    lm_results <- list()
    SPEI_cols <- colnames(x)[grep("^SPEI", colnames(x))]
    for(dv in dv_set) {
        
        for(w in spei_windows) {
            spei_past_col <- paste0("SPEI", w, "_y1")
            
            for(spei_cut in spei_cutoffs) {
                # Remove other potential independent variables
                this_x <- x[c(dv, union(predictors, SPEI_cols))]
                # Complete cases only
                this_x <- this_x[complete.cases(this_x),]

                message("dv = ", dv, " spei_cut = ",
                        spei_cut, " window = ", w, " n = ", nrow(this_x))

                # Identify observations not currently in a drought but that
                # WERE in a drought the previous year
                # PBE = potential Birch effect
                this_x$PBE = this_x$SPEI12 > 0 & this_x[spei_past_col] <= spei_cut

                # Drop non-predictors entirely now
                this_x <- this_x[c(dv, predictors, "PBE")]
                
                # Construct formula
                f <- as.formula(paste(dv, "~", lm_rhs_formula))

                # Do the rf k-fold
                rf_out <- do_k_fold(this_x, fit_and_test_rf)
                rf_out$model <- "rf"
                # Do the lm k-fold; do_k_fold will pass f on to fit_and_test_lm
                lm_out <- do_k_fold(this_x, test_lm_pbe, formula = f)
                lm_out$model <- "lm"
                info_cols <- tibble(spei_window = w,
                                    spei_cut = spei_cut,
                                    dv = dv,
                                    n = nrow(this_x),
                                    n_pbe_true = sum(this_x$PBE))
                lm_results[[paste(dv, w, spei_cut)]] <- 
                    bind_cols(info_cols, bind_rows(rf_out, lm_out))
            }
        }
    }
    bind_rows(lm_results)
}

out <- do_sensitivity_analysis(dat_pbe_base,
                               dv_set, predictors,
                               spei_windows = c(12, 24),
                               spei_cutoffs = spei_cutoffs)

# Compute means across the SPEI windows, drought definitions, and d.v.'s
out %>% 
    group_by(model, dv, spei_window, spei_cut, n, n_pbe_true) %>% 
    summarise(n_pbe_terms = mean(n_pbe_terms),
              n_signif_pbe_terms = mean(n_signif_pbe_terms),
              pbe_anova = mean(pbe_anova),
              pbe_importance = mean(pbe_importance),
              validation_r2 = round(mean(validation_r2, na.rm = TRUE), digits = 3),
              .groups = "drop") %>% 
    mutate(spei_cut = as.factor(spei_cut),
           spei_window = paste(spei_window, "months")) ->
    out_smry

DT::datatable(out_smry)

ggplot(out_smry %>% filter(validation_r2 > 0),
       aes(spei_window, spei_cut, fill = validation_r2)) +
    geom_tile() + 
    facet_grid(model ~ dv) +
    ggtitle("k-fold mean R2 by model type and dependent variable")

ggplot(out_smry %>% filter(model == "lm"),
       aes(spei_window, spei_cut, fill = pbe_anova < 0.05)) +
    geom_tile() + 
    facet_wrap(~dv) +
    ggtitle("lm two-model ANOVA: effect of including PBE")

# Table: number (and % of total) of TRUE PBE entries; how big are the datasets?
out_smry %>%
    # these numbers are the same for each model type; pick one
    filter(model == "lm") %>% 
    mutate(pbe_true = paste0(n_pbe_true, " (", round(n_pbe_true / n * 100, 0), "%)")) %>% 
    select(dv, spei_window, spei_cut, pbe_true) %>% 
    pivot_wider(names_from = "dv", values_from = "pbe_true") %>% 
    knitr::kable(caption = "How many 'PBE events' are present in the data?", align = "r")
```

**Conclusions:**

-   **PBE events (low SPEI the previous year, followed by non-drought conditions) occur in 0-10% of the data, depending on criteria.**

-   **In a linear model, an ANOVA comparing models with and without PBE is almost always significant for Rs_annual and Rh_annual w/ 12-month SPEI; the exceptions are at the most extreme drought cutoff value.**

## Conditional inference Random Forest

> Recall the default splitting rule during random forests tree building consists of selecting, out of all splits of the (randomly selected *mtry*) candidate variables, the split that minimizes the Gini impurity (in the case of classification) and the SSE (in case of regression). **However, [Strobl et al. (2007)](https://doi.org/10.1186/1471-2105-8-25) illustrated that these default splitting rules favor the selection of features with many possible splits (e.g., continuous variables or categorical variables with many categories) over variables with fewer splits (the extreme case being binary variables, which have only one possible split).** Conditional inference trees (Hothorn, Hornik, and Zeileis 2006) implement an alternative splitting mechanism that helps to reduce this variable selection bias. However, ensembling conditional inference trees has yet to be proven superior with regards to predictive accuracy and they take a lot longer to train.

From <https://bradleyboehmke.github.io/HOML/random-forest.html>.

Our `PBE` flag is a binary variable, so is its importance being
under-valued by `ranger`? Test a 
[conditional inference](https://link.springer.com/article/10.1186/1471-2105-9-307) 
tree-based forest (note, this is slow):

```{r conditional-inference}
#| cache: true

m_ci <- cforest(sqrt_Rs_annual ~ ., data = dat_pbe)
print(m_ci)
message("Variable importance of CI forest:")
sort(varimp(m_ci), decreasing = TRUE)

# Fit c.i. random forest model and test against validation data
# The independent variable is in the first column
fit_and_test_ci_rf <- function(x_train, x_val) {
    f <- as.formula(paste(colnames(x_train)[1], "~ ."))
    rf <- cforest(formula = f, data = x_train)
    train_preds <- predict(rf, newdata = NULL)
    training_r2 <- r2(train_preds, pull(x_train[1]))
    training_rmse <- rmse(train_preds, pull(x_train[1]))
    
    if(!is.null(x_val)) {
        val_preds <- predict(rf, newdata = x_val)
        val_obs <- pull(x_val[1])
        validation_r2 <- r2(preds = val_preds, obs = val_obs)
        validation_rmse <- rmse(preds = val_preds, obs = val_obs)
    } else {
        validation_r2 <- NA_real_
        validation_rmse <- NA_real_
    }
    
    tibble(training_r2 = training_r2, 
           training_rmse = training_rmse,
           validation_r2 = validation_r2,
           validation_rmse = validation_rmse)
}

do_k_fold(dat_pbe, fit_and_test_ci_rf) %>% 
    summarise(across(everything(), \(x) mean(x, digits = 3))) %>%
    mutate(k = K_FOLD)
```

**Conclusion: no evidence that conditional inference random 
forest produces different results.**

# Summary

Summary

# Reproducibility

```{r reproducibility}
#| echo: false

sessionInfo()
```
