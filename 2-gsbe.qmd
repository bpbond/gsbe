---
title: "2-gsbe"
author: "bbl"
format:
  html:
    embed-resources: true
    code-fold: true
toc: true
editor: visual
---

```{r setup}
#| include: false

K_FOLD <- 5

# Data processing and visualization packages
library(dplyr)
library(tidyr)
library(ggplot2)
theme_set(theme_bw())
library(readr)
library(lubridate)
library(arrow)

# Main analysis packages
library(ranger)
library(vivid)
library(broom)
library(car)

# Specialized packages used only for testing alternatives
library(party)
library(xgboost)
library(geodist)
library(spatialRF)

source("utilities.R")
```

# Background and science questions

(short background)

We ask four questions of increasing specificity:

1.  Does SPEI -- incorporating the statistical deviations of precipitation and evaporative demand in a single index -- add *Rs* predictive information beyond that of climate, LAI, vegetation, and remotely-derived productivity?

2.  Does the previous year's SPEI value have *Rs* predictive value? I.e., is there a memory effect?

3.  Specifically for *Rs* observations in a non-drought year *following one or more drought years*, does the previous year's SPEI have predictive value?

4.  Is there a 'Birch effect' in the observational record, in which non-drought *Rs* is higher than it otherwise would be when preceded by a drought?

# 0. Prep work and base models

```{r read-data}

dat <- read_csv("srdb_joined_data.csv", show_col_types = FALSE)

message(nrow(dat), " rows of data")

dat %>% 
    filter(is.na(Rs_growingseason) | (Rs_growingseason > 0 & Rs_growingseason < 20)) %>% 
    filter(is.na(Rs_annual) | (Rs_annual > 0 & Rs_annual < 4000)) %>% 
    filter(is.na(Rh_annual) | (Rh_annual > 0 & Rh_annual < 2000)) %>% 
    # Most SRDB records are from forests (~3000), then grasslands (~760), 
    # shrubland (~230), wetland (~210), and desert (~80). We add the next
    # one, savanna (~30), because it might be particularly vulnerable to 
    # a Birch effect, but group everything else into "Other"
    mutate(Ecosystem_type = if_else(Ecosystem_type %in% c("Desert", "Forest", "Grassland",
                                                          "Savanna", "Shrubland", "Wetland"),
                                    true = Ecosystem_type, 
                                    false = "Other",
                                    missing = "Other"),
           # vivid::pdpVars() below needs things as factors not char
           Ecosystem_type = as.factor(Ecosystem_type),
           Soil_type_name = as.factor(Soil_type_name)) ->
    dat_filtered

message("After filtering, ", nrow(dat_filtered), " rows of data")
```
## Collinearity

 `Tair`, `Tsoil_lev1`, and `Tsoil_lev2` are *highly* correlated
with each other (`see 1-gsbe-data-prep`), so drop level 2.

`VWC_lev1` and `VWC_lev2` are *highly* correlated with each
other (see `1-gsbe-data-prep`), so drop level 2.

## Dependent variable distribution

We have three potential dependent variables:

-   `Rs_annual` (annual soil respiration)

-   `Rh_annual` (annual heterotrophic respiration)

-   `Rs_growingseason` (mean growing season soil respiration)

Their distributions are expected to be non-normal, which is a problem for linear models and not ideal for anything. Evaluate possible transformations.

```{r iv-dist}
#| fig-width: 10
#| fig-height: 8

dat_filtered %>% 
    select(Rs_annual, Rh_annual, Rs_growingseason) %>% 
    pivot_longer(everything()) %>% 
    filter(!is.na(value), value > 0) %>% 
    rename(none = value) %>% 
    mutate(log = log(none), sqrt = sqrt(none)) %>% 
    
    pivot_longer(-name, names_to = "Transformation") %>% 
    ggplot(aes(value, color = Transformation)) + 
    geom_density() +
    facet_wrap(~ name + Transformation, scales = "free")

dat_filtered %>% 
    mutate(sqrt_Rs_annual = sqrt(Rs_annual),
           sqrt_Rh_annual = sqrt(Rh_annual),
           sqrt_Rs_growingseason = sqrt(Rs_growingseason)) ->
    dat_filtered
```

**Conclusion: we have a transformation winner: `sqrt()`!**

## Random Forest model: performance

We are using the `ranger` package.

Wright, M. N. and Ziegler, A.: Ranger: A fast implementation of random forests for high dimensional data in C++ and R, J. Stat. Softw., 77, https://doi.org/10.18637/jss.v077.i01, 2017.

```{r base-rf}
#| fig-width: 10
#| fig-height: 6
#| cache: true

# Fit random forest model and test against validation data
# The independent variable is in the first column
fit_and_test_rf <- function(x_train, x_val) {
    f <- as.formula(paste(colnames(x_train)[1], "~ ."))
    rf <- ranger::ranger(formula = f, data = x_train, importance = "impurity")
    # use importance = "permutation" for importance_pvalues()
    training_rmse <- rmse(predict(rf, data = x_train)$predictions, pull(x_train[1]))
        
    if(!is.null(x_val)) {
        val_preds <- predict(rf, data = x_val)$predictions
        val_obs <- pull(x_val[1])
        validation_r2 <- r2(preds = val_preds, obs = val_obs)
        validation_rmse <- rmse(preds = val_preds, obs = val_obs)
    } else {
        validation_r2 <- NA_real_
        validation_rmse <- NA_real_
    }
    
    return(tibble(training_r2 = rf$r.squared, 
                  training_rmse = training_rmse,
                  validation_r2 = validation_r2,
                  validation_rmse = validation_rmse))
}

# Base test dataset (all three d.v.'s)
dat_filtered %>% 
    select(sqrt_Rs_annual, sqrt_Rh_annual, sqrt_Rs_growingseason,
           # Tair, Tsoil_lev1, and Tsoil_lev2 are *highly* correlated
           # with each other (see 1-gsbe-data-prep), so drop level 2
           Tair, Tsoil_lev1, #Tsoil_lev2,
           # VWC_lev1 and VWC_lev2 are *highly* correlated with each
           # other (see 1-gsbe-data-prep), so drop level 2
           Precip, VWC_lev1, #VWC_lev2, 
           LAI_high, LAI_low, Ecosystem_type,
           starts_with("SPEI"),
           MODIS_NPP, MODIS_GPP,
           Veg_type_hi, Veg_type_low, Soil_type_name) ->
    dat_base_all

# Base test dataset without MODIS or SPEI data
dat_base_all %>% 
    select(-MODIS_NPP, -MODIS_GPP, -starts_with("SPEI")) ->
    dat_base_no_modispei

# Base test dataset (Rs_annual only)
dat_base_no_modispei %>% 
    select(-sqrt_Rh_annual, -sqrt_Rs_growingseason) %>% 
    filter(complete.cases(.)) -> 
    dat_base_Rs_annual

message("The test complete-cases dataset has ", dim(dat_base_Rs_annual)[1],
        " rows and ", dim(dat_base_Rs_annual)[2], " columns")
message("Full-data model performance:")
rf_all_data <- ranger(sqrt_Rs_annual ~ ., data = dat_base_Rs_annual, importance = "impurity")
print(rf_all_data)

message("Random forest predictor importance and Altmann p-values:")
imps <- sort(rf_all_data$variable.importance, decreasing = TRUE)
importance_pvalues(rf_all_data, "altmann", 
                   num.permutations = 100,
                   formula = sqrt_Rs_annual ~ ., 
                   data = dat_base_Rs_annual)

# Top three variables...
pdpVars(data = dat_base_Rs_annual, 
        fit = rf_all_data, 
        response = "sqrt_Rs_annual",
        vars = names(imps)[1:3])
# ...and the next three
pdpVars(data = dat_base_Rs_annual, 
        fit = rf_all_data, 
        response = "sqrt_Rs_annual",
        vars = names(imps)[4:6])

# k-fold cross-validation for all dependent variables
message("k-fold cross-validation:")

dvs <- c("sqrt_Rs_annual", "sqrt_Rh_annual", "sqrt_Rs_growingseason")
preds <- setdiff(colnames(dat_base_no_modispei), dvs)
kf_out <- list()
for(dv in dvs) {
    # Construct dataset and do the k-fold
    x <- dat_base_no_modispei[c(dv, preds)]
    x <- x[complete.cases(x),]
    kf_out[[dv]] <- do_k_fold(x, fit_and_test_rf) %>% mutate(dv = dv, n = nrow(x))
}
bind_rows(kf_out) %>% 
    group_by(dv) %>% 
    summarise(across(everything(), \(x) mean(x, digits = 3)), .groups = "drop") %>% 
    mutate(k = K_FOLD) -> # just so it's "10" or whatever as expected
    kf_out 
print(kf_out)

```

**Side note: model performance is linearly related to dataset size! Huh.**

```{r side-note}
ggplot(kf_out, aes(n, validation_r2)) + 
    geom_point() + 
    geom_smooth(method = lm, formula = y~x) +
    ggtitle("Validation R2 for Rs_annual, Rh_annual, and Rs_growingseason")
```

## How important are the MODIS data?

The MODIS data only start in 2001 (see `gsbe-data-prep` output), so requiring them in a model cuts off a lot of early data – on the order of 20% of the dataset. Are they needed?

```{r test-modis}
#| cache: true

dat_base_all %>% 
    select(-sqrt_Rh_annual, -sqrt_Rs_growingseason, -starts_with("SPEI")) %>% 
    filter(complete.cases(.)) -> 
    dat_base_test_modis

rf_modis <- ranger(sqrt_Rs_annual ~ ., data = dat_base_test_modis, importance = "impurity")
print(rf_modis)

message("Model *with* MODIS data: OOB R2 = ", round(rf_modis$r.squared, 4))
message("Model *without* MODIS data: OOB R2 = ", round(rf_all_data$r.squared, 4))

importance_pvalues(rf_modis, "altmann", 
                   num.permutations = 100,
                   formula = sqrt_Rs_annual ~ ., 
                   data = dat_base_test_modis)
```

**Conclusion: it doesn't seem like they're needed.**

## Roads not taken

### Test spatial Random Forest

Spatial autocorrelation wouldn't change the fit of the model, but it could 
bias the variable importance metrics. Evaluate.

```{r spatial-rf}
# This needs the newest version 1.1.5, not yet on CRAN
# remotes::install_github(repo = "blasbenito/spatialRF",
#   ref = "main", force = TRUE, quiet = TRUE)
stopifnot(packageVersion("spatialRF") >= "1.1.5")

dat_base_no_modispei %>% 
    bind_cols(select(dat_filtered, Longitude, Latitude)) %>% 
    select(-sqrt_Rh_annual, -sqrt_Rs_growingseason) %>% 
    filter(complete.cases(.)) -> 
    dat_spatial_test

# Compute distance between all pairs of points
dm <- geodist(select(dat_spatial_test, Longitude, Latitude), measure = "geodesic")
dat_spatial_test$Longitude <- dat_spatial_test$Latitude <- NULL
predictors <- setdiff(colnames(dat_spatial_test), colnames(dat_spatial_test)[1])
spatialRF::rf(data = dat_spatial_test, 
              dependent.variable.name = colnames(dat_spatial_test)[1],
              predictor.variable.names = predictors,
              distance.matrix = dm)
```

**Conclusion: no strong evidence of spatial autocorrelation.**

### Test XGBoost

Is there any reason to think that this popular algorithm would out-perform
our Random Forest approach, and/or give different answers?

```{r xgboost}
fit_and_test_xgboost <- function(x_train, x_val) {
    m_xgb <- xgboost(x_train[-1], x_train[1])
    pred_train <- predict(m_xgb, newdata = x_train)
    pred_val <- predict(m_xgb, newdata = x_val)
    tibble(training_r2 = r2(pred_train, pull(x_train[1])),
           validation_r2 = r2(pred_val, pull(x_val[1])))
}

m_xgb <- xgboost(dat_base_Rs_annual[-1], dat_base_Rs_annual[1])
message("Variable importance of XGBoost model:")
as_tibble(xgb.importance(m_xgb))

message("k-fold cross-validation results:")
do_k_fold(dat_base_Rs_annual, fit_and_test_xgboost) %>% 
    summary()
```

Seems like xgboost is over-fitting to the training data. Note however that

> Unlike random forests, GBMs can have high variability in accuracy dependent 
on their hyperparameter settings [(Probst, Bischl, and Boulesteix 2018)](https://arxiv.org/abs/1802.09596) So tuning can require much more 
strategy than a random forest model.

**Conclusion: performance and variable importance seem similar to `ranger`.**

### Test neural network

We use the `neuralnet` package for this. I experimented with the hidden layer
topology and threshold settings to find a configuration that reliably converges
with these data. Note: fitting this is *slow*.

```{r neuralnet}
#| cache: true
#| eval: false

# eval:false above b/c this is SO SLOW

library(neuralnet)

fit_and_test_nn <- function(x_train, x_val) {
    # Algorithm is not guaranteed to converge; if this happens, returns NAs
    out <- tibble(training_r2 = NA_real_, validation_r2 = NA_real_)
    try({
        m_nn <- neuralnet(sqrt_Rs_annual ~ ., data = x_train,
                          hidden = c(12, 6), rep = 2, threshold = 0.4)
        pred_train <- predict(m_nn, newdata = x_train)
        pred_val <- predict(m_nn, newdata = x_val)
        out <- tibble(training_r2 = r2(pred_train, pull(x_train[1])),
                      validation_r2 = r2(pred_val, pull(x_val[1])))
    })
    return(out)
}

# Rescale the variables and change factors to numeric
x <- dat_base_Rs_annual
nums <- sapply(x, is.numeric)
for(col in names(nums)[nums]) x[col] <- scale(x[col])
fcts <- sapply(x[-1], is.factor)
for(col in names(fcts)[fcts]) x[col] <- as.numeric(pull(x[col]))
#saveRDS(x, "nn_topology/x.RDS")

# Overall model
m_nn <- neuralnet(sqrt_Rs_annual ~ ., data = x, hidden = c(12, 6), rep = 2, threshold = 0.4)
message("R2 of two-layer model: ", round(r2(predict(m_nn, newdata = x), x$sqrt_Rs_annual), 3))
m_nn <- neuralnet(sqrt_Rs_annual ~ ., data = x, hidden = c(24, 12, 6), rep = 2, threshold = 0.4)
message("R2 of three-layer model: ", round(r2(predict(m_nn, newdata = x), x$sqrt_Rs_annual), 3))

message("k-fold cross-validation results:")
do_k_fold(x, fit_and_test_nn) %>% 
    summary()
```

With a complex topology (e.g., three hidden layers) the training R2
can be cranked up to 0.9+, but it exhibits severe overfitting.

**Conclusion: I'm not an expert here! But, no evidence that this
will be a magic bullet.**

## Linear regression model: performance

```{r base-lm}
#| fig-width: 10
#| fig-height: 6

# Fit random forest model and test against validation data
# The dependent variable is in the first column
fit_and_test_lm <- function(x_train, x_val, m) {
    pred_train <- predict(m, newdata = x_train)
    pred_val <- predict(m, newdata = x_val)
    tibble(training_r2 = r2(pred_train, pull(x_train[1])),
           training_rmse = rmse(pred_train, pull(x_train[1])),
           validation_r2 = r2(pred_val, pull(x_val[1])),
           validation_rmse = rmse(pred_val, pull(x_val[1])))
}

message("Fitting Rs_annual linear model...")
m_Rs <- lm(sqrt_Rs_annual ~ Tair + Tsoil_lev1 + 
            VWC_lev1 + I(VWC_lev1 ^ 2) + 
            Ecosystem_type + LAI_high * LAI_low + 
            Veg_type_hi + Veg_type_low + Soil_type_name,
        data = dat_base_Rs_annual)
m_Rs_reduced <- MASS::stepAIC(m_Rs, trace = FALSE)
summary(m_Rs_reduced)
par(mfrow = c(2, 2))
plot(m_Rs_reduced)

message("k-fold cross-validation results:")
lm_rhs_formula <- as.character(formula(m_Rs))[3]
kf_out <- list()
for(dv in dvs) {
    # Pull out the rhs of the formula and paste dv (dependent variable) in front
    f <- as.formula(paste(dv, "~", lm_rhs_formula))
    # Construct dataset and call lm() once to give us a base model
    x <- dat_base_no_modispei[c(dv, preds)]
    x <- x[complete.cases(x),]
    m <- lm(f, data = x)
    kf_out[[dv]] <- do_k_fold(x, f = fit_and_test_lm, m = m) %>% 
        mutate(dv = dv, n = nrow(x))
}
bind_rows(kf_out) %>% 
    group_by(dv) %>% 
    summarise(across(everything(), mean), .groups = "keep") %>% 
    mutate(across(everything(), round, digits = 3)) %>% 
    ungroup() %>% 
    mutate(k = K_FOLD) # just so it's "10" or whatever as expected

# Directly compare lm and rf predictions
tibble(Ecosystem_type = dat_base_Rs_annual$Ecosystem_type,
       lm_prediction = predict(m_Rs_reduced),
       rf_prediction = predict(rf_all_data, data = dat_base_Rs_annual)$predictions) %>% 
    ggplot(aes(lm_prediction, rf_prediction)) + 
    geom_point(alpha = 0.25) + 
    geom_abline() + 
    geom_smooth(method = "lm", formula = y ~ x) +
    ggtitle("Linear model versus Random Forest predictions") + 
    facet_wrap(~Ecosystem_type, scales = "free")
```

**Conclusions:**

-   **Linear model performance is less than half of Random Forest**

-   **Variable importance is similar**

# 1. Predictive value of SPEI

```{r spei-value}
#| cache: true

dat_base_all %>% 
    select(-sqrt_Rh_annual, -sqrt_Rs_growingseason,
           -MODIS_NPP, -MODIS_GPP,
           -starts_with("SPEI24")) %>% 
    filter(complete.cases(.)) -> 
    dat_test_spei

# remove the "_y1" columns
yr1cols <- grep("_y1$", colnames(dat_test_spei))
dat_test_spei_current <- dat_test_spei[-yr1cols]

message("Testing Random Forest...")
rf_spei <- ranger(sqrt_Rs_annual ~ ., data = dat_test_spei_current, importance = "impurity")
print(rf_spei)
importance_pvalues(rf_spei, "altmann", 
                   num.permutations = 100,
                   formula = sqrt_Rs_annual ~ ., 
                   data = dat_test_spei_current)

message("Model *with* SPEI data: OOB R2 = ", round(rf_spei$r.squared, 4))
message("Model *without* SPEI data: OOB R2 = ", round(rf_all_data$r.squared, 4))

message("Testing linear model...")
m_Rs_spei <- lm(sqrt_Rs_annual ~ (Tair + Tsoil_lev1 + 
            VWC_lev1 + I(VWC_lev1 ^ 2)) * 
               SPEI12 +
            Ecosystem_type + LAI_high * LAI_low + 
            Veg_type_hi + Veg_type_low + Soil_type_name,
        data = dat_test_spei_current)
m_Rs_spei_reduced <- MASS::stepAIC(m_Rs_spei, trace = FALSE)
car::Anova(m_Rs_spei_reduced, type = "III")

message("Model *with* SPEI data: training R2 = ", lm_r2(m_Rs_spei_reduced))
message("Model *without* SPEI data: training R2 = ", lm_r2(m_Rs_reduced))
message("ANOVA model comparison p-value: ", 
        anova_p_twomodels(m_Rs_spei_reduced, m_Rs_reduced))
```

**Conclusions:**

-   **SPEI impact on Random Forest model is small, but there is a performance improvement.**

-   **SPEI *does* have effects in the linear model: significant interactions with many variables, significant difference between models with and without SPEI, and model R2 jumps modestly.**

# 2. Predictive value of previous-year SPEI

```{r prev-spei-value}

# Use the dat_test_spei dataset from above

message("Testing Random Forest...")
rf_spei_y1 <- ranger(sqrt_Rs_annual ~ ., data = dat_test_spei, importance = "impurity")
print(rf_spei_y1)
message("Model *with* previous-year SPEI data: OOB R2 = ", round(rf_spei_y1$r.squared, 4))
message("Model *without* previous-year SPEI data: OOB R2 = ", round(rf_all_data$r.squared, 4))

importance_pvalues(rf_spei_y1, "altmann", 
                   num.permutations = 100,
                   formula = sqrt_Rs_annual ~ ., 
                   data = dat_test_spei)

message("Testing linear model...")
m_Rs_spei_y1 <- lm(sqrt_Rs_annual ~ (Tair + Tsoil_lev1 + 
            VWC_lev1 + I(VWC_lev1 ^ 2)) * 
               (SPEI12 + SPEI12_y1) +
            Ecosystem_type + LAI_high * LAI_low + 
            Veg_type_hi + Veg_type_low + Soil_type_name,
        data = dat_test_spei)
m_Rs_spei_y1_reduced <- MASS::stepAIC(m_Rs_spei_y1, trace = FALSE)
car::Anova(m_Rs_spei_y1_reduced, type = "III")

message("Model *with* previous-year SPEI data: training R2 = ", lm_r2(m_Rs_spei_y1_reduced))
message("Model *without* previous-year SPEI data: training R2 = ", lm_r2(m_Rs_spei_reduced))
message("ANOVA model comparison p-value: ", 
        anova_p_twomodels(m_Rs_spei_y1_reduced, m_Rs_spei_reduced))
```

**BUT there's a problem with the above.** 
Fit a model with *random* previous-year SPEI data:

```{r random-speiyr1}
dat_test_spei %>% 
    mutate(SPEI12_y1 = runif(n())) ->
    dat_test_spei_random

m_spei_random1 <- lm(formula(m_Rs_spei_y1), data = dat_test_spei_random)
m_spei_random1_reduced <- MASS::stepAIC(m_spei_random1, trace = FALSE)
car::Anova(m_spei_random1_reduced, type = "III")
```

...and some of the `SPEI_yr1` terms might still be significant, just by chance!

Use an ANOVA to compare the (reduced) models with and without previous-year SPEI:

```{r random-speiyr1-anova}
m_spei_random2 <- lm(sqrt_Rs_annual ~ (Tair + Tsoil_lev1 + 
            VWC_lev1 + I(VWC_lev1 ^ 2)) * 
               (SPEI12) +
            Ecosystem_type + LAI_high * LAI_low + 
            Veg_type_hi + Veg_type_low + Soil_type_name,
        data = dat_test_spei_random)
m_spei_random2_reduced <- MASS::stepAIC(m_spei_random2, trace = FALSE)

anova(m_spei_random1_reduced, m_spei_random2_reduced)
```

**Conclusions:**

-   **SPEI_y1 impact on Random Forest model is...unclear. No performance improvement.**

-   **SPEI_y1 has significant interactions with many variables in the linear model, model comparison ANOVA is significant, and model R2 increases slightly.**

-   **Need to be careful to include null model testing.**

# 3. Value of previous-year SPEI *after drought*

Similar to the previous section but focused on *post-drought observations* 
(was SPEI <= -1 the previous year but now, in year of Rs measurement, 
SPEI is >= 0).

```{r post-drought}
dat_test_spei %>% 
    filter(SPEI12 >= 0 & SPEI12_y1 <= -1) ->
    dat_test_post_drought
```

There are `r nrow(dat_test_post_drought)` Rs data points that meet these post-drought criteria.

```{r post-drought-spei}
message("Linear model:")
m_pd <- lm(formula = formula(m_Rs_spei), data = dat_test_post_drought)
m_pd_reduced <- MASS::stepAIC(m_pd, trace = FALSE)
m_pd_y1 <- lm(formula = formula(m_Rs_spei_y1), data = dat_test_post_drought)
m_pd_y1_reduced <- MASS::stepAIC(m_pd_y1, trace = FALSE)

message("Post-drought model *with* previous-year SPEI: training R2 = ", lm_r2(m_pd_y1_reduced))
message("Post-drought model *without* previous-year SPEI: training R2 = ", lm_r2(m_pd_reduced))
message("ANOVA model comparison p-value: ", 
        anova_p_twomodels(m_pd_y1_reduced, m_pd_reduced))

tibble(sqrt_Rs_annual = dat_test_post_drought$sqrt_Rs_annual, 
       WITHOUT_y1_SPEI = predict(m_pd),
       WITH_y1_SPEI = predict(m_pd_y1_reduced),
       Ecosystem_type = dat_test_post_drought$Ecosystem_type) %>% 
    pivot_longer(starts_with("WITH")) %>% 
    ggplot(aes(sqrt_Rs_annual, value, color = name)) +
    geom_point() + geom_abline() + 
    geom_smooth(method = lm, formula = y~x, se = FALSE, linetype = 2) +
    xlab("Observed Rs_annual") + ylab("Predicted Rs_annual")
```

Null test: random SPEI_y1.

```{r null-no-change}
dat_test_post_drought %>% 
    mutate(SPEI12_y1 = runif(n())) ->
    dat_test_post_drought_random

m_pd_random <- lm(formula = formula(m_pd_y1), data = dat_test_post_drought_random)
m_pd_random_reduced <- MASS::stepAIC(m_pd_random, trace = FALSE)

message("Post-drought model without previous-year SPEI: training R2 = ", lm_r2(m_pd_reduced))
message("Post-drought model with *random* previous-year SPEI: training R2 = ", lm_r2(m_pd_random_reduced))
message("ANOVA model comparison p-value: ", 
        anova_p_twomodels(m_pd_reduced, m_pd_random_reduced))
```

**Conclusions: for post-drought observations, previous-year SPEI1 has a big effect on prediction.**

# 4. Testing for a Birch effect

Are post-drought observations 'different'?

Define a PBE ("potential Birch effect") flag using the post-drought
conditions above: PBE <= -1 the previous year and PBE >0 = the year of
Rs measurement.

## Linear model: PBE importance and interactions

```{r test-linear}
#| fig-width: 10
#| fig-height: 6

# dat_test_spei is sqrt_Rs_annual plus climate predictors, SPEI12, and
# SPEI_y1
dat_test_spei %>% 
    mutate(PBE = SPEI12_y1 <= -1 & SPEI12 > 0) %>% 
    dplyr::select(-ends_with("_y1")) ->
    dat_pbe

message("Fitting Rs_annual linear model with PBE (SPEI12, -1.0) effect...")
m_Rs <- lm(sqrt_Rs_annual ~ (Tair + Tsoil_lev1 +
            VWC_lev1 + I(VWC_lev1 ^ 2)) * PBE +
            Ecosystem_type + LAI_high * LAI_low + 
            Veg_type_hi + Veg_type_low + Soil_type_name,
        data = dat_pbe)
m_Rs_reduced <- MASS::stepAIC(m_Rs, trace = FALSE)
car::Anova(m_Rs_reduced, type = "III")

message("Fitting Rs_annual linear model without PBE effect...")
m_Rs_no_PBE <- lm(sqrt_Rs_annual ~ (Tair + Tsoil_lev1 +
            VWC_lev1 + I(VWC_lev1 ^ 2)) +
            Ecosystem_type + LAI_high * LAI_low + 
            Veg_type_hi + Veg_type_low + Soil_type_name,
        data = dat_pbe)
m_Rs_no_PBE_reduced <- MASS::stepAIC(m_Rs_no_PBE, trace = FALSE)

message("Linear model without PBE flag: training R2 = ", lm_r2(m_Rs_no_PBE_reduced))
message("Linear model with PBE flag: training R2 = ", lm_r2(m_Rs_reduced))
message("ANOVA model comparison p-value: ", 
        anova_p_twomodels(m_Rs_reduced, m_Rs_no_PBE_reduced))

tibble(model_with_PBE = predict(m_Rs),
       model_without_PBE = predict(m_Rs_no_PBE),
       PBE = dat_pbe$PBE, 
       Ecosystem_type = dat_pbe$Ecosystem_type) %>% 
    ggplot(aes(model_without_PBE, model_with_PBE)) + 
    geom_point(aes(color = PBE)) + 
    geom_abline() + 
    facet_wrap(~Ecosystem_type) +
    ggtitle("Linear model: effect of including PBE on sqrt_Rs_annual")

# Extract the significant interactions between PBE and other variables 
message("Interactive PBE effects:")
tidy(m_Rs_reduced) %>% 
    mutate(model = "Rs_annual") %>% 
    filter(grepl("PBE", term)) %>% 
    separate(term, into = c("interaction", "PBE"), fill = "left", sep = ":") %>% 
    replace_na(list(interaction = "(none)")) %>% 
    select(model, interaction, estimate, p.value) %>% 
    mutate(estimate = round(estimate, 4), p.value = round(p.value, 4)) ->
    PBE_interactions

DT::datatable(PBE_interactions)

# Summary table of significant effect directions
PBE_interactions %>% 
    mutate(signif = p.value < 0.05, 
           effect = case_when(signif & estimate < 0 ~ "Negative",
                              signif & estimate >= 0 ~ "Positive",
                              .default = "")) %>% 
    select(model, interaction, effect) %>%
    pivot_wider(values_from = effect, names_from = model, values_fill = "") %>% 
    knitr::kable(caption = "PBE: significant effect directions")
```

**Conclusions:**

-   **PBE (potential Birch effect, i.e. having a strong drought the year before a well-watered year) has a strong statistical effect on the temperature and moisture drivers of soil respiration.**

-   **But, it's inconsistent—there's no clean 'Birch effect.'**

## PBE sensitivity test

```{r sensitivity-test}
#| eval: false

fit_and_test_lm <- function(x_train, x_val) {
    f <- as.formula(paste(colnames(x_train)[1], "~ (Tair + Tsoil_lev1 +",
                          "VWC_lev1 + I(VWC_lev1 ^ 2))",
                          "* PBE + Ecosystem_type + LAI_high * LAI_low +",
                          "Veg_type_hi + Veg_type_low + Soil_type_name"))

    m <- lm(formula = f, data = x_train)
    m_reduced <- MASS::stepAIC(m, trace = FALSE)
    
    if(!is.null(x_val)) {
        preds <- predict(m_reduced, newdata = x_val)
        obs <- pull(x_train[1])
        validation_r2 <- rs(preds = preds, obs = obs)
    } else {
        validation_r2 <- NA_real_
    }

    list(model = m_reduced,
         training_r2 = glance(m_reduced)$r.squared,
         validation_r2 = validation_r2,
         importance = tidy(m_reduced))
}

# These are the three most common fluxes recorded in SRDB
#iv_set <- c("Rs_annual")#, "Rh_annual", "Rs_growingseason")
iv_set <- c("sqrt_Rs_annual", "sqrt_Rh_annual", "sqrt_Rs_growingseason")

# The following SPEI values are from Table 1 of Keune et al. (2025)
# https://www.nature.com/articles/s41597-025-04896-y
# and correspond to "severely dry", "moderately dry", and "mildly dry"
# (there are no "extremely dry"=-2 in the dataset)
spei_cutoffs <- c(-1.5, -1.0, 0.0)

# What SPEI window should be used for the cutoffs above?
spei_windows <- c(12, 24) # months

# Model independent variables
predictors <- c("Ecosystem_type",
                "SPEI12",
                "Tair", "Precip",
                "Tsoil_lev1", "VWC_lev1", 
                "Veg_type_hi", "Veg_type_low", 
                "LAI_high", "LAI_low", "Soil_type_name")

do_full_analysis <- function(x, iv_set, predictors, spei_windows, spei_cutoffs) {
    rf_results <- list()
    lm_results <- list()
    SPEI_cols <- colnames(x)[grep("^SPEI", colnames(x))]
    for(iv in iv_set) {
        
        for(w in spei_windows) {
            spei_past_col <- paste0("SPEI", w, "_y1")
            
            for(spei_cut in spei_cutoffs) {
                # Remove other potential independent variables
                this_x <- x[c(iv, union(predictors, SPEI_cols))]
                # Complete cases only
                this_x <- this_x[complete.cases(this_x),]
                
                message("---------------------------------------------")
                message("iv = ", iv)
                message("spei_cut = ", spei_cut, " window = ", w)
                #message("\tn = ", nrow(this_x))
                
                # identify observations not currently in a drought but that
                # WERE in a drought the previous year
                # PBE = potential Birch effect
                this_x$PBE = this_x$SPEI12 > 0 & this_x[spei_past_col] <= spei_cut
                #message("\tPBE = ", sum(this_x$PBE))
                
                # Drop non-predictors entirely now
                this_x <- this_x[c(iv, predictors, "PBE")]
                
                # Fit models and save results
                rf_out <- fit_and_test_rf(this_x, NULL)
                rf_results[[paste(iv, w, spei_cut)]] <- 
                    tibble(model = "rf",
                           spei_window = w,
                           spei_cut = spei_cut,
                           iv = iv,
                           predictor = names(rf_out$importance),
                           n = nrow(this_x),
                           pbe_n = sum(this_x$PBE),
                           importance = rf_out$importance,
                           training_r2 = rf_out$training_r2)
                
                lm_out <- fit_and_test_lm(this_x, NULL)
                lm_results[[paste(iv, w, spei_cut)]] <- 
                    tibble(model = "lm",
                           spei_window = w,
                           spei_cut = spei_cut,
                           iv = iv,
                           predictor = lm_out$importance$term,
                           importance = lm_out$importance$p.value,
                           n = nrow(this_x),
                           pbe_n = sum(this_x$PBE),
                           training_r2 = lm_out$training_r2)
 
            }
        }
    }
    bind_rows(bind_rows(rf_results), bind_rows(lm_results))
}

out <- do_full_analysis(dat_filtered,
                        iv_set, predictors,
                        spei_windows = c(12,24),
                        spei_cutoffs = spei_cutoffs)

# Compute means across the SPEI windows, drought definitions, and i.v.'s
out %>% 
    group_by(model, spei_window, spei_cut, iv, n, pbe_n) %>% 
    summarise(r2 = round(mean(training_r2), digits = 3), .groups = "drop") %>% 
    mutate(spei_cut = as.factor(spei_cut),
           spei_window = paste(spei_window, "months")) ->
    out_smry

# Table: number (and % of total) of TRUE PBE entries; how big are the datasets?
out_smry %>%
    # these numbers are the same for each model type; pick one
    filter(model == "rf") %>% 
    mutate(pbe_n = paste0(pbe_n, " (", round(pbe_n / n * 100, 0), "%)")) %>% 
    select(spei_window, spei_cut, iv, pbe_n) %>% 
    pivot_wider(names_from = "iv", values_from = "pbe_n") %>% 
    knitr::kable(caption = "How often do 'PBE events' occur?", align = "r")

```

**Conclusions:**

-   **PBE events (low SPEI the previous year, followed by non-drought conditions) occur in 0-20% of the data, depending on criteria.**

## Random Forest sensitivity results

```{r rf-sensitivity}
#| eval: false

# Table: Training R2 values
out_smry %>%
    filter(model == "rf") %>% 
    select(spei_window, spei_cut, iv, r2) %>% 
    pivot_wider(names_from = "iv", values_from = "r2") %>% 
    knitr::kable(caption = "RF training (OOB) R2")

# Table: rank of PBE variable
out %>% 
    filter(model == "rf") %>% 
    group_by(spei_window, spei_cut, iv) %>% 
    mutate(rank = min_rank(desc(importance))) ->
    out_ranked

out_ranked %>% 
    filter(predictor == "PBE") %>% 
    select(spei_window, spei_cut, iv, rank) %>% 
    pivot_wider(names_from = "iv", values_from = "rank") %>%
    knitr::kable(caption = "Rank of PBE variable in RF model")

out_ranked %>% 
    group_by(iv, predictor) %>% summarise(rank = mean(rank)) %>% 
    ggplot(aes(iv, predictor, fill = rank)) + 
    geom_tile() + 
    xlab("Independent variable") +
    ggtitle("Predictor ranks in RF model")
```

**Conclusions:**

-   **The values used for SPEI window (12/24 months) and drought level (0, -1, -1.5) don't make much difference.**

-   **RF model explains 52-65% of OOB variability**

-   **Precip and Tair are uniformly important; LAI and soil temp/VWC medium importance; PBE (drought effect from previous year) not important**

## Linear model sensitivity results

```{r lm-sensitivity}
#| eval: false

# Table: Training R2 values
out_smry %>%
    filter(model == "lm") %>% 
    select(spei_window, spei_cut, iv, r2) %>% 
    pivot_wider(names_from = "iv", values_from = "r2") %>% 
    knitr::kable(caption = "lm training adjusted R2")

# Rank of significant variables
# We're using the p-value for a crude approach
out %>% 
    filter(model == "lm", importance < 0.05, grepl("PBE", predictor)) %>% 
    group_by(iv, predictor) %>% 
    mutate(n_signif = n()) %>% 
    ggplot(aes(iv, predictor, fill = n_signif)) + 
    geom_tile() + 
    xlab("Independent variable") +
    ggtitle("PBE significance counts in linear model")

message("PBE is significant by itself in a few models:")
out %>% 
    filter(predictor=="PBETRUE", model=="lm", importance < 0.05) %>% 
    select(-model, -training_r2)
```

**Conclusions:**

-   **The values used for SPEI window (12/24 months) and drought level (0, -1, -1.5) don't make much difference.**

-   **The linear model explains 16-27% of training data variability.**

-   **PBE significantly interacts with Tair (esp. in predicting Rs_annual), Tsoil, and VWC.**

## Conditional inference Random Forest

> Recall the default splitting rule during random forests tree building consists of selecting, out of all splits of the (randomly selected *mtry*) candidate variables, the split that minimizes the Gini impurity (in the case of classification) and the SSE (in case of regression). **However, [Strobl et al. (2007)](https://doi.org/10.1186/1471-2105-8-25) illustrated that these default splitting rules favor the selection of features with many possible splits (e.g., continuous variables or categorical variables with many categories) over variables with fewer splits (the extreme case being binary variables, which have only one possible split).** Conditional inference trees (Hothorn, Hornik, and Zeileis 2006) implement an alternative splitting mechanism that helps to reduce this variable selection bias. However, ensembling conditional inference trees has yet to be proven superior with regards to predictive accuracy and they take a lot longer to train.

From <https://bradleyboehmke.github.io/HOML/random-forest.html>.

Our `PBE` flag is a binary variable, so is its importance being under-valued by `ranger`? Test a conditional inference tree-based forest:

```{r conditional-inference}
#| eval: false

m_ci <- cforest(sqrt_Rs_annual ~ ., data = dat_pbe)
summary(m_ci)
message("Variable importance of CI forest:")
sort(varimp(m_ci), decreasing = TRUE)

message("TODO: k-fold validation R2")

```

**Conclusion: no evidence that conditional inference random forest produces different results.**

# Summary

Summary

# Reproducibility

```{r reproducibility}
#| echo: false

sessionInfo()
```
